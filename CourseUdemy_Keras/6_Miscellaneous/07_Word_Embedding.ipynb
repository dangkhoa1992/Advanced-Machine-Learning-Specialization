{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoding text\n",
    "- Assign each word into a number (mapping) -> input vector\n",
    "<img src=\"./Figs/1.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- Problems\n",
    "    + size of input = size of dictionary\n",
    "    + sparse input vector\n",
    "    + Lose relationship info. between words\n",
    "            -> Word embedding\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding (Word2Vec)\n",
    "- Represent 1 word = 1 vector\n",
    "- sparse vector -> dense vector\n",
    "<img src=\"./Figs/2.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- Semantic meaning: related words are grouped (near to each others)\n",
    "<img src=\"./Figs/3.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- sematic clouds\n",
    "<img src=\"./Figs/4.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "- Perform math operation on word vectors\n",
    "<img src=\"./Figs/5.jpg\" alt=\"Drawing\" style=\"width: 500px;\"/>\n",
    "\n",
    "### Train Word Embedding\n",
    "- Each word start randomly\n",
    "- vector values changed through backpropagation\n",
    "- similar words get closer during trainning\n",
    "- More dimensions \n",
    "    + more info. trained(gender, verbs, singular...)\n",
    "    + more trainning time\n",
    "    \n",
    "    \n",
    "## TSNE\n",
    "- Dimension reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 2)           200       \n",
      "=================================================================\n",
      "Total params: 200\n",
      "Trainable params: 200\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(\n",
    "    input_dim=100, # input = a number in range 0-99\n",
    "    output_dim=2)) # output = 2D vector\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = np.array(\n",
    "    [[81,  1, 96, 79],\n",
    "     [17, 47, 99, 50],\n",
    "     [0,  3, 12, 88]])\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Embedding = convert each number -> 2D vector\n",
    "#      Eg: 81 -> [ 0.03724655,  0.04417845]\n",
    "emb = model.predict(target)\n",
    "\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 1.8643711e-02,  3.4116950e-02],\n",
       "        [ 4.3568742e-02, -4.7537018e-02],\n",
       "        [-4.6716847e-02,  4.8901740e-02],\n",
       "        [ 7.2673932e-03,  1.9794814e-03]],\n",
       "\n",
       "       [[-2.9727830e-02, -3.1393647e-02],\n",
       "        [-3.2910693e-02, -2.6773661e-05],\n",
       "        [ 3.4771595e-02,  7.5961724e-03],\n",
       "        [ 5.9664249e-04,  1.6779136e-02]],\n",
       "\n",
       "       [[ 4.9195636e-02,  1.8332753e-02],\n",
       "        [-1.2222696e-02, -1.2293875e-02],\n",
       "        [ 4.1643355e-02,  2.4994340e-02],\n",
       "        [-6.9925897e-03, -2.0246617e-03]]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
