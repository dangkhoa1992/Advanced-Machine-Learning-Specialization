{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AUTOGRAD: AUTOMATIC DIFFERENTIATION\n",
    "- Autograd provides automatic differentiation for all operations on Tensors\n",
    "\n",
    "#### Tensor\n",
    "- `.requires_grad` = True\n",
    "\t+ Track all operations on it\n",
    "\t+ After finishing computations, can call `.backward()` to have all the gradients computed automatically\n",
    "\t+ The gradient for this tensor will be accumulated into `.grad` attribute\n",
    "\n",
    "- `.backward()`\n",
    "\t+ Compute the derivatives on a Tensor\n",
    "\t+ If Tensor is a scalar, donâ€™t need to specify any arguments to backward()\n",
    "\t+ If Tensor has more elements, need to specify a gradient argument that is a tensor of matching shape\n",
    "\n",
    "- `.detach()`:  to get a new Tensor with the same content but that does not require gradients\n",
    "\n",
    "- `with torch.no_grad():`\n",
    "\t+ To prevent tracking history (and using memory) of the code block\n",
    "\t+ Used when evaluating a model\n",
    "\n",
    "- `.grad_fn`\n",
    "\t+ Each tensor has a `.grad_fn` attribute that references a Function that has created the Tensor\n",
    "\t+ except for Tensors created by the user, `grad_fn` = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Change requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "tensor([[ 1.7315, -1.0468],\n",
      "        [-8.2165,  1.4425]], requires_grad=True)\n",
      "tensor(73.6851, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# User created tensor requires_grad = False\n",
    "a = torch.randn(2, 2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "\n",
    "# Change\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Detach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "tensor(True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "y = x.detach()\n",
    "\n",
    "print(y.requires_grad)\n",
    "print(x.eq(y).all())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with `.no_grad()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creat a computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(\n",
    "   [[1.0, 2.0],\n",
    "    [3.0, 4.0]], requires_grad=True)\n",
    "y = x + 2\n",
    "z = y**2 * 3\n",
    "out = z.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]], requires_grad=True)\n",
      "tensor([[3., 4.],\n",
      "        [5., 6.]], grad_fn=<AddBackward0>) <AddBackward0 object at 0x7fad3839af50>\n",
      "tensor([[ 27.,  48.],\n",
      "        [ 75., 108.]], grad_fn=<MulBackward0>)\n",
      "tensor(64.5000, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print(y, y.grad_fn)\n",
    "print(z)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate `x.grad`\n",
    "- **Forward**: $out = \\frac{1}{4} \\sum\\limits_i3(x_i + 2)^2$\n",
    "- **Backward**: $\\frac{\\partial\\ out_{i}}{\\partial\\ x_i} = \\frac{3}{2}(x_i + 2)$\n",
    "    + $\\frac{\\partial\\ out_{1}}{\\partial\\ x_1}\\bigr\\rvert_{x_1=1} = \\frac{3}{2}(x_1 + 2) = 4.5$\n",
    "    + $\\frac{\\partial\\ out_{2}}{\\partial\\ x_2}\\bigr\\rvert_{x_2=2} = \\frac{3}{2}(x_2 + 2) = 6.0$\n",
    "    + $\\frac{\\partial\\ out_{3}}{\\partial\\ x_3}\\bigr\\rvert_{x_3=3} = \\frac{3}{2}(x_3 + 2) = 7.5$\n",
    "    + $\\frac{\\partial\\ out_{4}}{\\partial\\ x_4}\\bigr\\rvert_{x_2=4} = \\frac{3}{2}(x_4 + 2) = 9.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 6.0000],\n",
      "        [7.5000, 9.0000]])\n"
     ]
    }
   ],
   "source": [
    "# d(out)/dx\n",
    "J = x.grad\n",
    "print(J)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
